{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A1. Write your own module for detecting the feature/attributefor the root note of a Decision Tree. Use Information gain as the impurity measure for identifying the root node. Assume that the features are categoricalor could be converted to categorical by binning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(y):\n",
    "    \"\"\"Calculate the entropy of a label array y.\"\"\"\n",
    "    class_labels, counts = np.unique(y, return_counts=True)\n",
    "    probabilities = counts / counts.sum()\n",
    "    return -np.sum(probabilities * np.log2(probabilities))\n",
    "\n",
    "def information_gain(X, y, feature_index):\n",
    "    \"\"\"Calculate the information gain of splitting on a given feature.\"\"\"\n",
    "    # Total entropy before split\n",
    "    total_entropy = entropy(y)\n",
    "    \n",
    "    # Values and counts of the feature to split on\n",
    "    values, counts = np.unique(X[:, feature_index], return_counts=True)\n",
    "    weighted_entropy = 0\n",
    "    \n",
    "    for value, count in zip(values, counts):\n",
    "        subset = y[X[:, feature_index] == value]\n",
    "        weighted_entropy += (count / y.size) * entropy(subset)\n",
    "    \n",
    "    # Information gain is total entropy minus the weighted entropy of the split\n",
    "    return total_entropy - weighted_entropy\n",
    "\n",
    "def find_best_feature(X, y):\n",
    "    \"\"\"Identify the best feature by information gain.\"\"\"\n",
    "    gains = [information_gain(X, y, feature) for feature in range(X.shape[1])]\n",
    "    return np.argmax(gains)\n",
    "\n",
    "# Example usage:\n",
    "# X = [[feature1, feature2, ...], [feature1, feature2, ...], ...]\n",
    "# y = [class1, class2, class1, class2, ...]\n",
    "# best_feature = find_best_feature(X, y)\n",
    "# print(\"Best feature index:\", best_feature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2. If the feature is continuous valued for A1, use equal widthor frequencybinning for converting the attribute to categorical valued.The binning type should be aparameter to the function built for binning. Write your own function for the binning task.The number of bins to be created should also be passed as aparameter to the function. Use function overloadingtoallow for usage of default parameters if no parameters are passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_continuous_feature(values, num_bins=3, method='width'):\n",
    "    \"\"\"Bin a continuous feature into categorical.\"\"\"\n",
    "    if method == 'width':\n",
    "        bins = np.linspace(np.min(values), np.max(values), num=num_bins+1)\n",
    "    elif method == 'frequency':\n",
    "        bins = np.percentile(values, np.linspace(0, 100, num=num_bins+1))\n",
    "    \n",
    "    categorical = np.digitize(values, bins) - 1  # Convert to zero-indexed categories\n",
    "    return categorical\n",
    "\n",
    "def categorical_binning(X, feature_index, num_bins=3, method='width'):\n",
    "    \"\"\"Apply binning to a specified feature in the dataset X.\"\"\"\n",
    "    X_binned = X.copy()\n",
    "    X_binned[:, feature_index] = bin_continuous_feature(X[:, feature_index], num_bins, method)\n",
    "    return X_binned\n",
    "\n",
    "# Example usage for a specific feature index and method\n",
    "# X_binned = categorical_binning(X, feature_index=0, num_bins=4, method='frequency')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A3. Expand the above functions to built your own Decision Tree module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeNode:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "def build_decision_tree(X, y, depth=0, max_depth=3):\n",
    "    if len(np.unique(y)) == 1 or depth == max_depth:\n",
    "        # If all targets are the same or max depth reached, return a leaf node\n",
    "        return DecisionTreeNode(value=np.bincount(y).argmax())\n",
    "    \n",
    "    best_feature = find_best_feature(X, y)\n",
    "    thresholds = np.unique(X[:, best_feature])\n",
    "    best_threshold = None\n",
    "    best_ig = -1\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        left_y = y[X[:, best_feature] <= threshold]\n",
    "        right_y = y[X[:, best_feature] > threshold]\n",
    "        ig = information_gain(X, y, best_feature)\n",
    "        if ig > best_ig:\n",
    "            best_ig = ig\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    if best_ig == 0:\n",
    "        return DecisionTreeNode(value=np.bincount(y).argmax())\n",
    "\n",
    "    left_subtree = build_decision_tree(X[X[:, best_feature] <= best_threshold], y[X[:, best_feature] <= best_threshold], depth+1, max_depth)\n",
    "    right_subtree = build_decision_tree(X[X[:, best_feature] > best_threshold], y[X[:, best_feature] > best_threshold], depth+1, max_depth)\n",
    "    \n",
    "    return DecisionTreeNode(feature=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
    "\n",
    "# Example of building a tree\n",
    "# tree = build_decision_tree(X, y, max_depth=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
