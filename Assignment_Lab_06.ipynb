{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V.Harish\n",
    "AIE22063"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A1. Develop the above perceptronin your own code (don’t use the perceptron model available from package). Use the initial weights as provided below. W0 = 10, W1 = 0.2, w2 = -0.75, learning rate (α) = 0.05\n",
    "Use Stepactivation function to learn the weights of the network to implement above provided AND gate logic. The activation function is demonstrated below.\n",
    "Identify the number of epochs needed for the weights to converge in the learning process. \n",
    "Make a plot of the epochs against the error values calculated (after each epoch, calculate the sum-square-error against all training samples).\n",
    "\n",
    "A2. Repeat the above A1 experiment with following activation functions. Compare the iterations \n",
    "taken to converge against each of the activation functions. Keep the learning rate same as A1.\n",
    "• Bi-Polar Step function\n",
    "• Sigmoid function\n",
    "• ReLU function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 99\u001b[0m\n\u001b[0;32m     97\u001b[0m W2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.75\u001b[39m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Train the perceptron with the activation function\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m W0, W1, W2, epochs, errors \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActivation function:\u001b[39m\u001b[38;5;124m\"\u001b[39m, activation\u001b[38;5;241m.\u001b[39m_name_)\n",
      "Cell \u001b[1;32mIn[15], line 61\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(activation)\u001b[0m\n\u001b[0;32m     59\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Train the perceptron until the error is zero\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;66;03m# Increment the number of epochs\u001b[39;00m\n\u001b[0;32m     63\u001b[0m   epochs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     64\u001b[0m   \u001b[38;5;66;03m# Store the current error\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 45\u001b[0m, in \u001b[0;36merror\u001b[1;34m(activation)\u001b[0m\n\u001b[0;32m     43\u001b[0m T \u001b[38;5;241m=\u001b[39m outputs[i]\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Get the perceptron output\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m Z \u001b[38;5;241m=\u001b[39m \u001b[43mperceptron\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Calculate the squared error\u001b[39;00m\n\u001b[0;32m     47\u001b[0m squared_error \u001b[38;5;241m=\u001b[39m (T \u001b[38;5;241m-\u001b[39m Z) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "Cell \u001b[1;32mIn[15], line 30\u001b[0m, in \u001b[0;36mperceptron\u001b[1;34m(A, B, activation)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Define the perceptron function\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperceptron\u001b[39m(A, B, activation):\n\u001b[0;32m     31\u001b[0m   \u001b[38;5;66;03m# Compute the weighted sum\u001b[39;00m\n\u001b[0;32m     32\u001b[0m   weighted_sum \u001b[38;5;241m=\u001b[39m W0 \u001b[38;5;241m+\u001b[39m W1 \u001b[38;5;241m*\u001b[39m A \u001b[38;5;241m+\u001b[39m W2 \u001b[38;5;241m*\u001b[39m B\n\u001b[0;32m     33\u001b[0m   \u001b[38;5;66;03m# Apply the activation function\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the inputs and outputs for the AND gate\n",
    "inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "outputs = [0, 0, 0, 1]\n",
    "\n",
    "# Define the initial weights and learning rate\n",
    "W0 = 10\n",
    "W1 = 0.2\n",
    "W2 = -0.75\n",
    "alpha = 0.05\n",
    "\n",
    "# Define the activation functions\n",
    "import math\n",
    "\n",
    "def bipolar_step(x):\n",
    "  if x < 0:\n",
    "    return -1\n",
    "  else:\n",
    "    return 1\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "  if x < 0:\n",
    "    return 0\n",
    "  else:\n",
    "    return x\n",
    "\n",
    "# Define the perceptron function\n",
    "def perceptron(A, B, activation):\n",
    "  # Compute the weighted sum\n",
    "  weighted_sum = W0 + W1 * A + W2 * B\n",
    "  # Apply the activation function\n",
    "  output = activation(weighted_sum)\n",
    "  return output\n",
    "\n",
    "# Define a function to calculate the sum of squared errors\n",
    "def error(activation):\n",
    "  total_error = 0\n",
    "  for i in range(len(inputs)):\n",
    "    # Get the input and output pair\n",
    "    A, B = inputs[i]\n",
    "    T = outputs[i]\n",
    "    # Get the perceptron output\n",
    "    Z = perceptron(A, B, activation)\n",
    "    # Calculate the squared error\n",
    "    squared_error = (T - Z) ** 2\n",
    "    # Add to the total error\n",
    "    total_error += squared_error\n",
    "  return total_error\n",
    "\n",
    "# Define a function to train the perceptron with a given activation function\n",
    "def train(activation):\n",
    "  # Define global variables for the weights\n",
    "  global W0, W1, W2\n",
    "  # Define a list to store the errors for each epoch\n",
    "  errors = []\n",
    "  # Define a variable to track the number of epochs\n",
    "  epochs = 0\n",
    "  # Train the perceptron until the error is zero\n",
    "  while error(activation) > 0:\n",
    "    # Increment the number of epochs\n",
    "    epochs += 1\n",
    "    # Store the current error\n",
    "    errors.append(error(activation))\n",
    "    # Loop through the input and output pairs\n",
    "    for i in range(len(inputs)):\n",
    "      # Get the input and output pair\n",
    "      A, B = inputs[i]\n",
    "      T = outputs[i]\n",
    "      # Get the perceptron output\n",
    "      Z = perceptron(A, B, activation)\n",
    "      # Update the weights\n",
    "      if activation == bipolar_step:\n",
    "        W0 = W0 + alpha * (T - Z) * 1\n",
    "        W1 = W1 + alpha * (T - Z) * A\n",
    "        W2 = W2 + alpha * (T - Z) * B\n",
    "      elif activation == sigmoid:\n",
    "        W0 = W0 + alpha * (T - Z) * Z * (1 - Z) * 1\n",
    "        W1 = W1 + alpha * (T - Z) * Z * (1 - Z) * A\n",
    "        W2 = W2 + alpha * (T - Z) * Z * (1 - Z) * B\n",
    "      elif activation == relu:\n",
    "        W0 = W0 + alpha * (T - Z) * 1\n",
    "        W1 = W1 + alpha * (T - Z) * A\n",
    "        W2 = W2 + alpha * (T - Z) * B\n",
    "  # Return the final weights, number of epochs, and errors\n",
    "  return W0, W1, W2, epochs, errors\n",
    "\n",
    "# Define a list of activation functions\n",
    "activations = [bipolar_step, sigmoid, relu]\n",
    "\n",
    "# Loop through the activation functions\n",
    "for activation in activations:\n",
    "  # Reset the weights to the initial values\n",
    "  W0 = 10\n",
    "  W1 = 0.2\n",
    "  W2 = -0.75\n",
    "  # Train the perceptron with the activation function\n",
    "  W0, W1, W2, epochs, errors = train(activation)\n",
    "  # Print the results\n",
    "  print(\"Activation function:\", activation._name_)\n",
    "  print(\"Final weights:\")\n",
    "  print(\"W0 =\", W0)\n",
    "  print(\"W1 =\", W1)\n",
    "  print(\"W2 =\", W2)\n",
    "  print(\"Number of epochs:\", epochs)\n",
    "  # Plot the errors against the epochs\n",
    "  import matplotlib.pyplot as plt\n",
    "  plt.plot(range(1, epochs + 1), errors)\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Error\")\n",
    "  plt.title(\"Perceptron Learning Curve with \" + activation._name_)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A3. Repeat exercise A1 with varying the learning rate, keeping the initial weights same. Take learning \n",
    "rate = {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1}. Make a plot of the number of iterations taken for \n",
    "learning to converge against the learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the inputs and outputs for the AND gate\n",
    "inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "outputs = [0, 0, 0, 1]\n",
    "\n",
    "# Define the initial weights\n",
    "W0 = 10\n",
    "W1 = 0.2\n",
    "W2 = -0.75\n",
    "\n",
    "# Define the step function\n",
    "def step(x):\n",
    "  if x < 0:\n",
    "    return 0\n",
    "  else:\n",
    "    return 1\n",
    "\n",
    "# Define the perceptron function\n",
    "def perceptron(A, B):\n",
    "  # Compute the weighted sum\n",
    "  weighted_sum = W0 + W1 * A + W2 * B\n",
    "  # Apply the step function\n",
    "  output = step(weighted_sum)\n",
    "  return output\n",
    "\n",
    "# Define a function to calculate the sum of squared errors\n",
    "def error():\n",
    "  total_error = 0\n",
    "  for i in range(len(inputs)):\n",
    "    # Get the input and output pair\n",
    "    A, B = inputs[i]\n",
    "    T = outputs[i]\n",
    "    # Get the perceptron output\n",
    "    Z = perceptron(A, B)\n",
    "    # Calculate the squared error\n",
    "    squared_error = (T - Z) ** 2\n",
    "    # Add to the total error\n",
    "    total_error += squared_error\n",
    "  return total_error\n",
    "\n",
    "# Define a function to train the perceptron with a given learning rate\n",
    "def train(alpha):\n",
    "  # Define global variables for the weights\n",
    "  global W0, W1, W2\n",
    "  # Define a variable to track the number of epochs\n",
    "  epochs = 0\n",
    "  # Train the perceptron until the error is zero\n",
    "  while error() > 0:\n",
    "    # Increment the number of epochs\n",
    "    epochs += 1\n",
    "    # Loop through the input and output pairs\n",
    "    for i in range(len(inputs)):\n",
    "      # Get the input and output pair\n",
    "      A, B = inputs[i]\n",
    "      T = outputs[i]\n",
    "      # Get the perceptron output\n",
    "      Z = perceptron(A, B)\n",
    "      # Update the weights\n",
    "      W0 = W0 + alpha * (T - Z) * 1\n",
    "      W1 = W1 + alpha * (T - Z) * A\n",
    "      W2 = W2 + alpha * (T - Z) * B\n",
    "  # Return the number of epochs\n",
    "  return epochs\n",
    "\n",
    "# Define a list of learning rates\n",
    "learning_rates = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "# Define a list to store the number of epochs for each learning rate\n",
    "epochs_list = []\n",
    "\n",
    "# Loop through the learning rates\n",
    "for alpha in learning_rates:\n",
    "  # Reset the weights to the initial values\n",
    "  W0 = 10\n",
    "  W1 = 0.2\n",
    "  W2 = -0.75\n",
    "  # Train the perceptron with the learning rate\n",
    "  epochs = train(alpha)\n",
    "  # Store the number of epochs\n",
    "  epochs_list.append(epochs)\n",
    "  # Print the results\n",
    "  print(\"Learning rate:\", alpha)\n",
    "  print(\"Number of epochs:\", epochs)\n",
    "\n",
    "# Plot the number of epochs against the learning rates\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(learning_rates, epochs_list)\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Number of epochs\")\n",
    "plt.title(\"Perceptron Learning Rate vs Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
