{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2. Use cross-validation techniques (RandomizedSearchCV()) technique to tune the hyperparameters for your perceptron and MLP networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'epochs': 500, 'learning_rate': 0.057133826761273626}\n",
      "Accuracy of best perceptron on test data: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class Perceptron(BaseEstimator):\n",
    "    def __init__(self, num_features, learning_rate=0.01, epochs=1000):\n",
    "        self.num_features = num_features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
    "        return self.sigmoid(weighted_sum)\n",
    "\n",
    "    def train(self, inputs, labels):\n",
    "        self.weights = np.random.rand(self.num_features)\n",
    "        self.bias = np.random.rand()\n",
    "        for _ in range(self.epochs):\n",
    "            for x, y in zip(inputs, labels):\n",
    "                prediction = self.predict(x)\n",
    "                error = y - prediction\n",
    "                self.weights += self.learning_rate * error * x\n",
    "                self.bias += self.learning_rate * error\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.train(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.where(self.sigmoid(np.dot(X, self.weights) + self.bias) >= 0.5, 1, 0)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'num_features': self.num_features,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'epochs': self.epochs\n",
    "        }\n",
    "\n",
    "# Load data from Excel sheet into DataFrame\n",
    "df = pd.read_excel(\"C:\\\\Users\\\\HarishVReddy\\\\Downloads\\\\customerdata.xlsx\")\n",
    "\n",
    "# Encoding labels to numeric values\n",
    "label_encoding = {'Yes': 1, 'No': 0}\n",
    "df['High Value Tx'] = df['High Value Tx'].map(label_encoding)\n",
    "\n",
    "# Extracting features and labels\n",
    "inputs = df.drop(columns=['Customer', 'High Value Tx']).values.astype(float)\n",
    "labels = df['High Value Tx'].values\n",
    "\n",
    "# Normalize inputs\n",
    "inputs = inputs / inputs.max(axis=0)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': uniform(0.001, 0.1),\n",
    "    'epochs': [100, 500, 1000, 2000]\n",
    "}\n",
    "\n",
    "# Create an instance of the Perceptron class\n",
    "perceptron = Perceptron(num_features=X_train.shape[1])\n",
    "\n",
    "# Create RandomizedSearchCV instance with a dummy scoring function\n",
    "random_search = RandomizedSearchCV(estimator=perceptron, param_distributions=param_grid, n_iter=10, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit RandomizedSearchCV to training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Initialize perceptron with best parameters\n",
    "best_perceptron = Perceptron(num_features=X_train.shape[1], **best_params)\n",
    "\n",
    "\n",
    "# Train the perceptron with best parameters\n",
    "best_perceptron.fit(X_train, y_train)\n",
    "\n",
    "# Test the perceptron with best parameters on test data\n",
    "y_pred = best_perceptron.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of best perceptron on test data: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A3. Tabulate your results with various other classifiers such as Support Vector Machines, Decision Tree, RandomForest, CatBoost, AdaBoost, XGBoost, Naïve-Bayes. Tabulate your results for your problem using different performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier, AdaBoostClassifier\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaive_bayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GaussianNB\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CatBoostClassifier\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score, f1_score\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_classifier(classifier, X, y):\n",
    "    accuracy = cross_val_score(classifier, X, y, cv=5, scoring='accuracy').mean()\n",
    "    precision = cross_val_score(classifier, X, y, cv=5, scoring='precision').mean()\n",
    "    recall = cross_val_score(classifier, X, y, cv=5, scoring='recall').mean()\n",
    "    f1 = cross_val_score(classifier, X, y, cv=5, scoring='f1').mean()\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Load data from Excel sheet into DataFrame\n",
    "df = pd.read_excel(\"C:\\\\Users\\\\HarishVReddy\\\\Downloads\\\\customerdata.xlsx\")\n",
    "\n",
    "# Encoding labels to numeric values\n",
    "label_encoding = {'Yes': 1, 'No': 0}\n",
    "df['High Value Tx'] = df['High Value Tx'].map(label_encoding)\n",
    "\n",
    "# Extracting features and labels\n",
    "inputs = df.drop(columns=['Customer', 'High Value Tx']).values.astype(float)\n",
    "labels = df['High Value Tx'].values\n",
    "\n",
    "# Normalize inputs\n",
    "inputs = inputs / inputs.max(axis=0)\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = {\n",
    "    \"Support Vector Machine\": SVC(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"Naïve Bayes\": GaussianNB(),\n",
    "    \"CatBoost\": CatBoostClassifier(logging_level='Silent'),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Evaluate each classifier\n",
    "results = {}\n",
    "for clf_name, clf in classifiers.items():\n",
    "    accuracy, precision, recall, f1 = evaluate_classifier(clf, inputs, labels)\n",
    "    results[clf_name] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1 Score': f1}\n",
    "\n",
    "# Display results in a tabular format\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
